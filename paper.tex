\documentclass[english,listof=totoc]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[authoryear]{natbib}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=false]
 {hyperref}\usepackage{mathtools}

\title{ML Estimation of Matrix--$t$ Distributions with Expectation Maximization}


\author{Zahari Kassabov\thanks{TIF-UNIMI-2016-5}\\
        Dipartimento di Fisica, Universit\`a di Torino and INFN, Sezione di Torino\\
		TIF Lab, Dipartimento di Fisica, Universit\`a di Milano\\
        \texttt{kassabov@to.infn.it}\\
        \and
        Hsien-Ching Kao\\
        Wolfram Research, Inc.\\
		Champaign, Illinois 61820, USA\\
		\texttt{sp000088@gmail.com}
		}

\begin{document}

\maketitle

\begin{abstract}
We provide an expectation-maximization (EM) algorithm for the maximum
likelihood estimation of matrix--$t$ distributions and discuss its
practical implementation. The result can be applied to problems where
the parameters of a matrix--$t$ distribution need to be obtained from
data, such as the design of recommendation systems and remote sensing
algorithms.
\end{abstract}

\section{Introduction}
The matrix--$t$ distribution is a generalization of the multivariate
$t$ distribution to matrix variate domains. Similar to other $t$
distributions, the matrix--$t$ distribution is a scale mixture of
a particular class of multivariate normal models: In particular, it is
a mixture of a matrix normal distribution, with a matrix-valued scale
parameter \citep{gupta1999matrix}.  Because of its mixture nature, the
matrix--$t$ distribution can be applied to building predictive models
with applications in tasks such as  spatial interpolation (for
example, to predict the concentration of pollution
\citep{KIBRIA2006785}) and recommendation systems
\citep{NIPS2007_3203}. Matrix models can be advantageous compared to
more traditional PCA methods because the correlation structure among
different observations can be captured in the scale matrix parameters,
and hence do not require the $i.i.d$ assumption
\citep{2013arXiv1309.6609G}. In such inferential problems, it is
required to obtain the parameters of the underlying model, given a set
of observed data samples. For matrix--$t$ distributions, the
parameters consist of a degrees of freedom parameter, a matrix
location parameter, and two scale matrix parameters.

The method of maximum likelihood (ML) is a common approach to estimate
the parameters of the distribution. However, for matrix--$t$
distributions, optimizing the likelihood directly is impractical since
the gradient equations are unwieldy. Instead, we can exploit the fact
that general for $t$ distributions, it is well-known that one can use
the mixture property and apply the expectation maximization (EM)
technique to optimize the likelihood function
\citep{10.2307/24305551}. EM can also be  applied to the particular
case of a matrix variate domains, by viewing the matrix--$t$
distribution as a mixture.

The related problem of determining a sample from a matrix--$t$
distribution given some observed entries of the matrix and parameters
of the model was studied in \cite{NIPS2007_3203}.

This paper is organized as follows: In section \ref{sec:matrixt} we
give a brief review on matrix--$t$ distributions, fix the notation and
give some matrix calculus results. In section \ref{sec:EM}, we derive
the EM formulae and provide relevant implementation details.  A brief
discussion is given in section \ref{sec:discussion}.

\section{Matrix--$t$ distributions}\label{sec:matrixt}

All matrices referred in this paper are real. The matrix determinant
is denoted as $|\cdot|$. We use the symbol $\mathbf{S}_{++}^p$ to
indicate the set of symmetric positive definite (SPD) matrices of
dimension $p$. The multivariate gamma function $\Gamma_{p}(\cdot)$,
given in terms of the standard gamma function $\Gamma(\cdot)$, is
%
\begin{equation}
\Gamma_{p}(x)=\pi^{p(p-1)/4}\prod_{j=1}^{p}\Gamma\left(x+\frac{1-j}{2}\right)
\ .\label{eq:multgammadef}
\end{equation}
%
We also define the multivariate digamma function
%
\begin{equation}
\psi_{p}(x):=\frac{d}{dx}\log\Gamma_{p}(x)=%
\sum_{j=1}^{p}\psi\left(x+\frac{1-j}{2}\right)\ .
\end{equation}

A $p
\times m$ random matrix $T$ is said to be matrix--$t$ distributed if
given the scale parameter $S$, a SPD matrix of dimensions $p\times p$,
the matrix $T$ is matrix normal distributed $N_{p,m}(M,S,\Omega)$ and
$S$ is itself inverse Wishart distributed $W^{-1}(\Sigma,\nu +p -1)$:
%
\begin{equation}
T\,|\,M,\Sigma,\Omega,\nu,S \sim N_{p,m}(M,S,\Omega) \ ,\label{eq:tdef1}
\end{equation}
and
\begin{equation}
S\,|\,\Sigma,\nu \sim W^{-1}(\Sigma,\nu+p-1)\ .\label{eq:tdef2}
\end{equation}
%
Therefore, the matrix--$t$ distribution $T_{p,m}(M,\Sigma,\Omega,\nu)$
of dimensions $p\times m$ is specified by 4 parameters: the location
matrix $M$ of dimensions $p\times m$ , the row and column SPD scale
matrices $\Sigma$ and $\Omega$ of dimensions $p\times p$ and $m\times
m$, respectively, and the degrees of freedom parameter $\nu>0$.

The density of the matrix normal distribution $N_{p,m}(M,S,\Omega)$
\begin{equation}
\frac{1}{(2\pi)^{pm/2}|\Omega|^{p/2}|S|^{m/2}}%
\exp\left(-\frac{1}{2}\textrm{tr}\left[\Omega^{-1}(X-M)^{\textrm{t}}S^{-1}(X-M)\right]\right)
\label{eq:ndistpdf}
\end{equation}
at $X$,  while the density of the inverse Wishart distribution
$W^{-1}(\Sigma,n)$ with $n>p-1$ is
\begin{equation}
\frac{|\Sigma|^{\frac{n}{2}}}{2^{\frac{n p}{2}}%
|S|^{\frac{n+p+1}{2}}\Gamma_{p}(\frac{n}{2})}%
\exp\left(-\frac{1}{2}\textrm{tr}\left[\Sigma S^{-1}\right]\right)
\label{eq:wdistpdf}
\end{equation}
at $S$. Here $\Gamma_{p}(\cdot)$ is the multivariate gamma function
defined in Eq.~\eqref{eq:multgammadef}.

The density of the matrix--$t$ distribution can be obtained by
computing the joint density of $S$ and $T$ taken as independent
variables $f\left(T,S; (M,\Sigma,\Omega,\nu)\right)$, as the product
of the matrix normal and inverse Wishart densities
Eqs.~\eqref{eq:ndistpdf} and \eqref{eq:wdistpdf}, and then integrating
the hidden parameter $S$  over the field of SPD matrices, as follows
from the definitions in Eqs.~\eqref{eq:tdef1} and \eqref{eq:tdef2}.

The density function
$f(T;\left\{M,\Sigma,\Omega,\nu\right\})$ of a matrix--$t$
distribution $T_{p,m}(M,\Sigma,\Omega,\nu)$ satisfies
\begin{equation}
\frac{\Gamma_{p}\left(\frac{p+\nu+m-1}{2}\right)}%
{\Gamma_{p}\left(\frac{\nu+p-1}{2}\right)%
\pi^{\frac{mp}{2}}|\Omega|^{\frac{p}{2}}|\Sigma|^{\frac{m}{2}}}%
|I_p+\Sigma^{-1}(T-M)\Omega^{-1}(T-M)^{\textrm{t}}|^{\frac{1-m-p-\nu}{2}}
\label{eq:matrixtpdf}
\end{equation}
%
with $I_p$ the identity matrix of dimension $p$.

The marginalization
over $S$ can be obtained by using the Jacobain determinant of an
inverse SPD matrix $U=S^{-1}$
%
\begin{equation}
\left|\frac{\partial U}{\partial S}\right|=|S|^{-p-1}
\end{equation}
and the integral identity \citep{gupta1999matrix}
\begin{equation}
\int_{\mathbf{S}_{++}^p}e^{-\textrm{tr}[S A]}|S|^{\alpha}\,dS=%
\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}}
\ .\label{eq:intmultgammadef}
\end{equation}
Where $A$ is a $p\times p$ nonsingular matrix. The result in
eq.~\eqref{eq:matrixtpdf} then follows by direct substitution.


Similar to matrix normal distributions, matrix--$t$ distributions are
defined up to a multiplicative constant. It is easy to see from the
density function \eqref{eq:matrixtpdf} that
$T_{p,m}(M,c\,\Omega,\Sigma/c,\nu)$ has the same distribution as
$T_{p,m}(M,\Omega,\Sigma,\nu)$ for any constant $c>0$. This property
needs to be taken into account in an iterative ML estimation algorithm
in order to be able to reach convergence.


\section{EM formulae for the matrix--$t$ distribution}\label{sec:EM}

We begin by explaining how the EM method can be used to determine the
maximum likelihood parameters for some model from a set of
observations: Consider a random variable $X$ with probability density
(or mass) function $p_{\theta}(\cdot)$. The EM method applies if the
density is a marginalization over some unobserved random state $Z$,
i.e., $p_{\theta}(x)=\mathbb{E}_{\theta}\left[p_{\theta}(x|Z)\right]$.
Given a set of observations $\left\{x_j\right\}_{j\in J}$, the
classical EM method for ML estimation consists of two steps:
%
\begin{itemize}
\item At the $k$-th iteration, compute the conditional expectations
	with respect to $Z$ for each observation $x_j$ \begin{equation}
Q_i(\theta,\theta_k)=\mathbb{E}_{\theta_k}\left[\log p_{\theta}(x_i,Z)|x_i\right].
\end{equation}

\item Update $\theta$ by maximizing $Q(\theta,\theta_k):=\sum_{j\in J}Q_j(\theta,\theta_k)$ with respect to $\theta$, i.e.,
\begin{equation}
\theta_{k+1} = \textrm{argmax}_{\theta}\,Q(\theta,\theta_k).
\end{equation}
\end{itemize}
The algorithm is initialized by providing some initial guess for the
parameters of interest, $\theta_0$.

We now show how the EM method is applied to matrix--$t$ distributions.

\subsection{Expectation step}

Given observed matrix data $\left\{T_j\right\}_{j\in J}$ (with a total
of $|J|$ observed matrix instances), for each observation $T_j$ the
conditional expectation at step $k$ is
%
\begin{equation}
Q_j(\theta,\theta_k)=f(T_j;\theta_k)^{-1}\int_{\mathbf{S}_{++}^p}%
f(T_{j},S;\theta_k)\log f\left(T_{j},S;\theta\right)\,dS%
,\label{eq:expdef}
\end{equation}
where $\theta=\left\{M,\Sigma,\Omega,\nu\right\}$,
$\theta_k=\left\{M_k,\Sigma_k,\Omega_k,\nu_k\right\}$, and
$f(T,S;\theta)$ is the full joint likelihood of the distribution with
the hidden scale parameter $S$.

We can split the integral in Eq.~\eqref{eq:expdef} by the terms of
$\log f(T_{j},S;\theta)$. Eliminating a global factor $\frac{1}{2}$
and constant terms which do not depend on the parameters in $\theta$
we have
\begin{equation}
\begin{split}\log f(T_{j},S;\theta)\sim -\nu\log|S|-\textrm{tr}\left[S^{-1}\left\{\Sigma+(T_j-M)\Omega^{-1}(T_j-M)^{\textrm{t}}\right\}\right]\\
-\nu p\log2-p\log |\Omega|+(\nu+p-1)\log |\Sigma|-2\log\Gamma_{p}\left(\frac{\nu+p-1}{2}\right).
\end{split}\label{eq:logf}
\end{equation}
The first two terms in Eq.~\ref{eq:logf} require computing the
following integrals:
\begin{equation}
\int_{\mathbf{S}_{++}^p}f(T_j,S;\theta_k)\log |S|\,dS, \qquad
\int_{\mathbf{S}_{++}^p}f(T_j,S;\theta_k) S^{-1}\,dS.
\end{equation}
Note that the trace operation commutes with the matrix integration in
the for the second term.  The integrals can be computed from
identities obtained by taking the derivative of
Eq.~\eqref{eq:intmultgammadef} with respect to $\alpha$ and matrix
$A$, respectively:
%
\begin{equation}
\int_{\mathbf{S}_{++}^p}e^{-\textrm{tr}\left[SA\right]}|S|^{\alpha}\log |S|\,dS=\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}}\left\{\gamma_{p}\left(\alpha+\frac{1+p}{2}\right)-\log |A|\right\},\label{eq:intderivative1}
\end{equation}
\begin{equation}
\int_{\mathbf{S}_{++}^p}e^{-\textrm{tr}\left[SA\right]}|S|^{\alpha}S\,dS=\left(\alpha+\frac{1+p}{2}\right)\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}}A^{-1}.\label{eq:intderivative2}
\end{equation}
For the rest of the terms in Eq.~\eqref{eq:logf}, the integration over
$S$ simply cancels out the prefactor.

Thus, using the equalities Eq.~\eqref{eq:intderivative1} and
Eq.~\eqref{eq:intderivative2}, and after some rearrangements, we
obtain the closed form expression of $Q_j(\theta,\theta_k)$, where we
have dropped the irrelevant constant terms
%
\begin{equation}
\begin{split}Q_j(\theta,\theta_k) \sim \nu\left\{\log|W_j^{(k)}|+\psi_{p}\left(\frac{m+\nu_k+p-1}{2}\right)\right\}-2\log\Gamma_{p}\left(\frac{\nu+p-1}{2}\right)-p\log|\Omega|\\
-(m+\nu_k+p-1)\textrm{tr}\left[W_j^{(k)}\left\{\Sigma+(T_j-M)\Omega^{-1}(T_j-M)^{\textrm{t}}\right\}\right]+(\nu+p-1)\log |\Sigma|.
\end{split}
\label{eq:fsimp}
\end{equation}
Here the matrices $\left\{W_j^{(k)}\right\}_{j\in J}$ are defined as
\begin{equation}
W_j^{(k)}:=\left\{\Sigma_k+(T_j-M_k)\Omega_k^{-1}(T_j-M_k)^{\textrm{t}}\right\}^{-1}.
\end{equation}
Since $\Sigma_k$ and $\Omega_k$ are both SPD, each matrix $W_j^{(k)}$
is also SPD.

The main computational challenge in our algorithm is calculating the
$W_j^{(k)}$ matrices efficiently. We employ Cholesky factorization via
LAPACK and the BLAS routines for symmetric
matrix update. When $p$ is much greater than $m$, i.e., the dimension
of $\Sigma$ is much larger than $\Omega$, one can apply the
Sherman--Morrison--Woodbury formulae \citep{doi:10.1137/1023004} to
reduce the cost of inverting the $p\times p$ matrix for each
$W_j^{(k)}$. By using the formulae $W_j^{(k)}$ can be written as
%
\begin{equation}
W_j^{(k)}=\Sigma_k^{-1}-\Sigma_k^{-1}U_j\left(I_m+U_j^{\textrm{t}}\Sigma_k^{-1}U_j\right)^{-1}U_j^{\textrm{t}}\Sigma_k^{-1}\label{eq:SMW}
\end{equation}
with $U_j=(T_j-M_k)\Omega_k^{-1/2}$ and $\Omega_k^{1/2}$ is the upper
Cholesky factor of $\Omega_k$. Note that in Eq.~\eqref{eq:SMW} one
inverts a $m\times m$ matrix for each $j$ and the inversion (or
factorization) of $\Sigma_k$ only needs to be done once in each
iteration.

\subsection{Maximization step}

To maximize $Q(\theta,\theta_k)$ with respect to $\theta$ we compute
the gradient of each parameter and equate them to $0$ to obtain
$\theta_{k+1}$. For parameter $M$ we have
\begin{equation}
\sum_{j\in J}W_j^{(k)}\left(T_j-M\right)\Omega^{-1}=0.
\end{equation}
As $\Omega$ is nonsingular we obtain $M_{k+1}$ as an average of the
input data weighted by $W_j^{(k)}$
\begin{equation}
M_{k+1}=\left(\sum_{j\in J}W_j^{(k)}\right)^{-1}\sum_{j\in J}W_j^{(k)}T_j.\label{eq:M}
\end{equation}
Upon maximizing $Q(\theta,\theta_k)$ with respect to $\Omega$, we obtain
$\Omega_{k+1}$
\begin{equation}
\Omega_{k+1}=\frac{m+\nu_k+p-1}{p}\left(\frac{1}{|J|}\sum_{j\in J}(T_j-M_{k+1})^{\textrm{t}}W_j^{(k)}(T_j-M_{k+1})\right).\label{eq:Omega}
\end{equation}
For the rest of the parameter(s), we consider two possible situations.
\begin{itemize}
\item $\Sigma$ and $\nu$ are unknown: In this situation, maximizing
	$Q(\theta,\theta_k)$ with respect to $\Sigma$ we obtain
	$\Sigma_{k+1}$ in terms of the inverse of the mean of
	$W_j^{{(k)}}$
\begin{equation}
\Sigma_{k+1}(\nu)=\frac{\nu+p-1}{m+\nu_k+p-1}\left(\bar{W}^{(k)}\right)^{-1},\quad \bar{W}^{(k)}:=\frac{1}{|J|}\sum_{j\in J}W_j^{(k)}.\label{eq:Sigma}
\end{equation}
$\Sigma_{k+1}$ depends linearly on $\nu$. Due to this, we substitute
$\Sigma_{k+1}$ back into $Q(\theta,\theta_k)$ and maximize it with
respect to $\nu$. We note that
\[\tilde{Q}(\nu):=Q\left(\left\{M_{k+1},\Sigma_{k+1}(\nu),\Omega_{k+1},\nu\right\},\theta_k\right)\]
is a concave function of $\nu$ for all $\nu>0$. In this
situation we obtain $\nu_{k+1}$ by solving the equation
$\tilde{Q}'(\nu)=0$ which has the form
\begin{equation}
h(\nu+p-1)-h(m+\nu_k+p-1)+\frac{1}{|J|}\sum_{j\in J}\log|W_j^{(k)}|-\log |\bar{W}^{(k)}|=0\label{eq:nu1}
\end{equation}
with $h(x):=p\log(x)-\psi_p(x/2)$.
\item $\Sigma$ is known and $\nu$ is unknown: In this situation, $Q\left(\left\{M_{k+1},\Sigma,\Omega_{k+1},\nu\right\},\theta_k\right)$ is also a concave function of $\nu$. The gradient equation of $\nu$ is given by
\begin{equation}
\log|\Sigma|-\log |\bar{W}^{(k)}|+\psi_{p}\left(\frac{m+\nu_k+p-1}{2}\right)-\psi_{p}\left(\frac{\nu+p-1}{2}\right)=0.\label{eq:nu2}
\end{equation}
\end{itemize}

When $\Omega$ and $\Sigma$ are both unknown, the updating steps given
in Eqs.~\eqref{eq:Omega} and \eqref{eq:Sigma} increases the likelihood
of matrix--$t$ distribution monotonically but the parameters
$\Omega_k$ and $\Sigma_k$ may not converge due to the
non-identifiability of matrix--$t$ mentioned at the end of section
\ref{sec:matrixt}. This issue has also been discussed in
\citep{2013arXiv1309.6609G} for ML estimation of matrix normal
distributions. However, instead of fixing a single component in one of
the scale matrices, we suggest setting the mean trace ratio
\begin{equation}
\frac{\textrm{tr}[\Omega_k]/m}{\textrm{tr}[\Sigma_k]/p}
\end{equation}
of $\Omega_k$ and $\Sigma_k$ to 1 by proper scaling at the end of each
M-step. This ensures the convergence of the algorithm and also avoids
possible numerical issues due to the possible large scale separation
between $\Omega_k$ and $\Sigma_k$ in the iterative steps.

\subsection{Parameters initialization}

In the EM method, the initial values of the parameters have
a significant effect on the convergence behavior, since the algorithm
does not guarantee reaching the global optimum. This is less of an
issue for matrix--$t$ distributions because the ML estimation is
a convex optimization problem in the parameters
$\left\{M,\Sigma^{-1},\Omega^{-1},\nu\right\}$, but it is still
desirable to select good initial values to reduce the number of EM
iterations necessary to reach the required accuracy as the classical
log EM is a first order algorithm.

For the location parameter $M$, we found that the component-wise
median, and spatial
median \citep{Vardi15022000} with distance induced by Frobenius norm
\begin{equation}
d(T,T')=\sqrt{\textrm{tr}\left[(T-T')(T-T')^{\textrm{t}}\right]}
\end{equation}
are good choices. Both estimators can be computed with time complexity
$O(Nmp)$ with $N=|J|$ the number of matrices in the given data.

To obtain the rest of initial the parameters, we first center the data
by the initial values of $M$. We then  use the property of low
dimensional projection of matrix--$t$ distributions to obtain the
initial values.  It is known that if $T\sim
T_{p,m}(\mathbf{0},\Sigma,\Omega,\nu)$, then $Tv$ is a multivariate
$t$ distributed with scale parameter $\left(\frac{v^{\textrm{t}}\Omega
v}{\nu}\right)\Sigma$ and the same degrees of freedom parameter $\nu$
for any nonzero $m$ dimensional vector $v$ \citep{gupta1999matrix}.
This allows using existing routines for estimating multivariate $t$
distributions to retrieve the initial values.

\section{Discussion}\label{sec:discussion}

In this paper we derive and provide the EM formulae for ML estimation
of matrix--$t$ distributions. The EM algorithm here mainly consists of
computing the weight matrices $W_j^{(k)}$ and we provide some guidance
on its numerical implementation. A method of choosing the initial
values for the parameters in EM is briefly discussed.

In principle, the computation burden of estimating matrix--$t$
distributions can be reduced significantly with the EM algorithm
discussed here. However, due to the lack of optimized routines which
dedicate to linear algebra computations of matrix-valued vector
components, the algorithm described here can be difficult to optimize
in the actual implementation. Also, scalability with large data can be
an issue as the computation involves dense matrix inversion for each
observation during the EM iterations. It is possible to alleviate the
cost by considering matrix--$t$ distributions with structured scale
matrix parameters which allow fast factorization. The EM relations
described here can still be applied but with some modifications in the
M-step. The application of structured scale matrix parameters in
matrix--$t$ inference will be future work.

\section*{Acknowledgement}
Z.K. is supported by  the  Executive  Research  Agency  (REA)  of  the
European Commission under the Grant Agreement PITN-GA-2012-316704
(HiggsTools).

\bibliographystyle{unsrtnat}
\bibliography{references}
\end{document}
