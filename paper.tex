\documentclass[english,listof=totoc]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[authoryear]{natbib}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=false]
 {hyperref}\usepackage{mathtools}

\title{ML Estimation of Matrix--$t$ Distributions with Expectation Maximization}


\author{Zahari Kassabov\thanks{TIF-UNIMI-2016-5}\\
        Dipartimento di Fisica, Universit\`a di Torino and INFN, Sezione di Torino\\
		TIF Lab, Dipartimento di Fisica, Universit\`a di Milano\\
        \texttt{kassabov@to.infn.it}\\
        \and
        Hsien-Ching Kao\\
        Wolfram Research, Inc.\\
		Champaign, Illinois 61820, USA\\
		\texttt{sp000088@gmail.com}
		}

\begin{document}

\maketitle

\begin{abstract}
We provide an expectation-maximization (EM) formulae for the maximum likelihood estimation of matrix--$t$ distributions and discuss their practical implementation. The result can be applied to problems where the parameters of a matrix--$t$ distribution need to be obtained from data, such as the design of recommendation systems and remote sensing algorithms.
\end{abstract}

\section{Introduction}
Matrix--$t$ distribution is a generalization of multivariate $t$ distribution to matrix variates situations. Similar as other $t$ distributions, matrix--$t$ distribution is a scale mixture of a particular class of multivariate normal models, matrix normal distributions, except the mixture parameter is a matrix rather than a scalar \citep{gupta1999matrix}. Due to this, the distribution shares the importance in robust estimation and has been applied to many predictive tasks such as spatial interpolation on the prediction of pollution concentration \citep{KIBRIA2006785}, recommendation systems \citep{NIPS2007_3203}, and ... In these problems, it is necessary to obtain the parameters of the distribution, which consist of a degrees of freedom parameter, a matrix location parameter, and two scale matrix parameters, that describe a set of given matrix--valued observations for further inference.

A common approach on estimating the parameters is through the method of maximum likelihood (ML). However, optimizing the likelihood directly is a formidable task as the gradient equations are unwieldy slow to compute and solve. For $t$ distributions, it is well-known that one can exploit the property of scale mixture and apply expectation maximization (EM) technique to optimize the likelihood function \citep{10.2307/24305551}. The same trick can be also applied to matrix--$t$ distribution by viewing it as a matrix-normal distribution with inverse Wishart distributed scale parameter.

This note is organized as follows. In section \ref{sec:derivation}, we present an EM formulae for MLE of matrix--$t$ distribution together with the relevant details on the derivation. The notation and the preliminary results on matrix calculus needed to present the derivation are presented sequentially in the section. Practical implementation of the algorithm is discussed in section \ref{sec:implement} and a brief conclusion is given in section \ref{sec:conclusion}.

\section{EM for matrix--$t$ distribution}\label{sec:derivation}

All matrices referred in this note are real. For notation convenience, the matrix determinant is denoted as $|\cdot|$ and $\mathbf{S}_+^p$ is used to indicate the set of symmetric positive definite (SPD) matrices of dimension $p$. The multivariate gamma
function, $\Gamma_{p}(\cdot)$, given in terms of the standard gamma function $\Gamma(\cdot)$ has the form
\begin{equation}
\Gamma_{p}(x)=\pi^{p(p-1)/4}\prod_{j=1}^{p}\Gamma\left(x+\frac{1-j}{2}\right).\label{eq:multgammadef}
\end{equation}
We also define the multivariate digamma function
%
\begin{equation}
\psi_{p}(x):=\frac{\partial}{\partial x}\log\Gamma_{p}(x)=\sum_{j=1}^{p}\psi\left(x+\frac{1-j}{2}\right).
\end{equation}

\subsection{Matrix--$t$ distribution}

A matrix--$t$ distribution $T_{p,m}(M,\Sigma,\Omega,\nu)$ of dimensions $p\times m$ is specified by 4 parameters: the location matrix $M$ of dimensions $p\times m$ , the row and column SPD scale matrices $\Sigma$ and $\Omega$ of dimensions $p\times p$ and $m\times m$, respectively, and the degrees of freedom parameter $\nu>0$. A $p \times m$ random matrix $T$ is said to be matrix--$t$ distributed if given the scale parameter $S$, a SPD matrix of dimensions $p\times p$, the matrix $T$ is matrix normal distributed $N_{p,m}(M,S,\Omega)$ and $S$ is itself inverse Wishart distributed $W^{-1}(\Sigma,\nu +p -1)$:
\begin{equation}
T\,|\,M,\Sigma,\Omega,\nu,S \sim N_{p,m}(M,S,\Omega)\label{eq:tdef1}
\end{equation}
and
\begin{equation}
S\,|\,\Sigma,\nu \sim W^{-1}(\Sigma,\nu+p-1).\label{eq:tdef2}
\end{equation}
The matrix normal distribution $N_{p,m}(M,S,\Omega)$ has density
\begin{equation}
\frac{1}{(2\pi)^{pm/2}|\Omega|^{p/2}|S|^{m/2}}\exp\left(-\frac{1}{2}\textrm{tr}\left[\Omega^{-1}(X-M)^{\textrm{t}}S^{-1}(X-M)\right]\right)\label{eq:ndistpdf}
\end{equation}
at $X$ while the inverse Wishart distribution $W^{-1}(\Sigma,n)$ with $n>p-1$ has density
\begin{equation}
\frac{|\Sigma|^{\frac{n}{2}}}{2^{\frac{n p}{2}}|S|^{\frac{n+p+1}{2}}\Gamma_{p}(\frac{n}{2})}\exp\left(-\frac{1}{2}\textrm{tr}\left[\Sigma S^{-1}\right]\right)\label{eq:wdistpdf}
\end{equation}
at $S$. Here $\Gamma_{p}(\cdot)$ is the
multivariate gamma function defined in Eq.~\eqref{eq:multgammadef}.

Follow the definition in Eqs.~\eqref{eq:tdef1} and \eqref{eq:tdef2} and the densities given above we can see the density function $f(T;\left\{M,\Sigma,\Omega,\nu\right\})$ of a matrix--$t$ distribution $T_{p,m}(M,\Sigma,\Omega,\nu)$ satisfies
\begin{equation}
\frac{\Gamma_{p}\left(\frac{p+\nu+m-1}{2}\right)}{\Gamma_{p}\left(\frac{\nu+p-1}{2}\right)\pi^{\frac{mp}{2}}|\Omega|^{\frac{p}{2}}|\Sigma|^{\frac{m}{2}}}
|I_p+\Sigma^{-1}(T-M)\Omega^{-1}(T-M)^{\textrm{t}}|^{\frac{1-m-p-\nu}{2}}
\label{eq:matrixtpdf}
\end{equation}
with $I_p$ the identity matrix of dimension $p$. The marginalization over $S$ can be obtained by using the Jacobain determinant for inverse SPD matrix $U=S^{-1}$
\begin{equation}
\left|\frac{\partial U}{\partial S}\right|=|S|^{-p-1}
\end{equation}
and the integral identity \citep{gupta1999matrix}
\begin{equation}
\int_{\mathbf{S}_+^p}e^{-\textrm{tr}[S A]}|S|^{\alpha}\,dS=\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}},\label{eq:intmultgammadef}
\end{equation}
where $A$ is a $p\times p$ nonsingular matrix.

Similar as matrix normal distributions, matrix--$t$ distributions are defined up to a multiplicative constant, i.e., $T_{p,m}(M,c\,\Omega,\Sigma/c,\nu)$ has the same distribution as $T_{p,m}(M,\Omega,\Sigma,\nu)$ for any constant $c>0$.

\subsection{EM formulas for matrix--$t$ distribution}

Given a random variable $X$ with probability distribution with density (or mass) function $p_{\theta}(\cdot)$ with $\theta$ dependence. Assume the density (or mass) function is a marginalization over some unobserved random state $Z$, i.e., $p_{\theta}(x)=\mathbb{E}_{\theta}\left[p_{\theta}(x|Z)\right]$. With observations $\left\{x_i\right\}_{i\in I}$ the classical EM method of MLE consists of two steps:

\begin{itemize}
\item At the $k$-th iteration, compute the conditional expectations with respect to $Z$
\begin{equation}
Q(\theta,\theta_k)=\sum_{i\in I}Q_i(\theta,\theta_k),\qquad Q_i(\theta,\theta_k)=\mathbb{E}_{\theta_k}\left[\log p_{\theta}(x_i,Z)|x_i\right].
\end{equation}

\item Update $\theta$ by maximizing $Q(\theta,\theta_k)$, i.e., $\theta_{k+1} = \textrm{argmax}_{\theta}\,Q(\theta,\theta_k)$.
\end{itemize}

Given observed matrix data $\left\{T_j\right\}_{j\in J}$, for each observation $T_j$ the E-step of matrix--$t$ distributions entails computing the following integral at step $k$
\begin{equation}
Q_j(\theta,\theta_k)=f(T_j;\theta_k)^{-1}\int_{\mathbf{S}_+^p}f(T_{j},S;\theta_k)\log f\left(T_{j},S;\theta\right)\,dS,\label{eq:expdef}
\end{equation}
where $\theta=\left\{M,\Sigma,\Omega,\nu\right\}$, $\theta_k=\left\{M_k,\Sigma_k,\Omega_k,\nu_k\right\}$, and $f(T,S;\theta)$ is the full likelihood of the matrix--$t$ distribution with the hidden scale parameter $S$.

We can split the integral in Eq.~\eqref{eq:expdef} by the terms of $\log f(T_{j},S;\theta)$. Eliminating a global factor $\frac{1}{2}$ and terms which do not depend on the parameters in $\theta$ we have
\begin{equation}
\begin{split}\log f(T_{j},S;\theta)\sim -\nu\log|S|-\textrm{tr}\left[S^{-1}\left\{\Sigma+(T_j-M)\Omega^{-1}(T_j-M)^{\textrm{t}}\right\}\right]\\
-\nu p\log2-p\log |\Omega|+(\nu+p-1)\log |\Sigma|-2\log\Gamma_{p}\left(\frac{\nu+p-1}{2}\right).
\end{split}\label{eq:logf}
\end{equation}
The integrals to be computed are
\begin{equation}
\int_{\mathbf{S}_+^p}f(T_j,S;\theta_k)\log |S|\,dS, \qquad 
\int_{\mathbf{S}_+^p}f(T_j,S;\theta_k) S^{-1}\,dS
\end{equation}
and they can be done by using the equalities derived by taking the derivative of Eq.~\eqref{eq:intmultgammadef} with respect to $\alpha$ and matrix $A$, respectively,
\begin{equation}
\int_{\mathbf{S}_+^p}e^{-\textrm{tr}\left[SA\right]}|S|^{\alpha}\log |S|\,dS=\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}}\left\{\gamma_{p}\left(\alpha+\frac{1+p}{2}\right)-\log |A|\right\},\label{eq:intderivative1}
\end{equation}
\begin{equation}
\int_{\mathbf{S}_+^p}e^{-\textrm{tr}\left[SA\right]}|S|^{\alpha}S\,dS=\left(\alpha+\frac{1+p}{2}\right)\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}}A^{-1}.\label{eq:intderivative2}
\end{equation}

Utilizing the equalities \eqref{eq:intderivative1} and \eqref{eq:intderivative2} with adequate rearrangements we obtain the closed form expression of $Q_j(\theta,\theta_k)$ while ignoring the irrelevant constant terms
\begin{equation}
\begin{split}Q_j(\theta,\theta_k) \sim \nu\psi_{p}\left(\frac{m+\nu_k+p-1}{2}\right)-2\log\Gamma_{p}\left(\frac{\nu+p-1}{2}\right)-p\log|\Omega|\\
-\nu\log|\Sigma_k+(T_j-M_k)\Omega_k^{-1}(T_j-M_k)^{\textrm{t}}|+(\nu+p-1)\log |\Sigma|\\
-(m+\nu_k+p-1)\textrm{tr}\left[\left\{\Sigma_k+(T_j-M_k)\Omega_k^{-1}(T_j-M_k)^{\textrm{t}}\right\}^{-1}\right.\\
\left.\left\{(T_j-M)\Omega^{-1}(T_j-M)^{\textrm{t}}+\Sigma\right\}\right].
\end{split}
\label{eq:fsimp}
\end{equation}
The maximization formulae follows by setting the gradient of the parameters to zero.

For $M_{t}$ we have:
\begin{equation}
\frac{dE}{dM_{t}}=0\Rightarrow\sum_{i}^{N}\left((T_{i}-M)\Omega^{-1}(T_{i}-M)^{\textrm{t}}+\Sigma\right)^{-1}\left(T_{i}-M_{t}\right)\Omega_{t}^{-1}=0
\end{equation}
and because $\Omega_{t}$ is nonsingular, we can obtain $M_{t}$ as
a weighted average of the input data:
\begin{equation}
M_{t}=\frac{1}{N}\sum_{i}^{N}\left((T_{i}-M)\Omega^{-1}(T_{i}-M)^{\textrm{t}}+\Sigma\right)^{-1}T_{i}=\frac{1}{N}\sum_{i}^{N}W_{i}T_{i}
\end{equation}
where we have defined the weight matrices $W_{i}$ as
\begin{equation}
W_{i}=\left((T_{i}-M)\Omega^{-1}(T_{i}-M)^{\textrm{t}}+\Sigma\right)^{-1}
\end{equation}

$\Sigma_{t}$ is given in terms of the inverse of the mean weight
by:
\begin{equation}
\Sigma_{t}=\frac{n_{t}+p-1}{n+m+p-1}\left[\frac{1}{N}\sum_{i}^{N}W_{i}\right]^{-1}
\end{equation}
Note that it depends on $n_{t}$.

For $\Omega_{t}$ we obtain:
\begin{equation}
\Omega_{t}=\frac{n+p+m-1}{p}\frac{1}{N}\sum_{i}^{N}(T_{i}-M_{t})W_{i}(T_{i}-M_{t})
\end{equation}
and for $n_{t}$, in the case where $\Sigma_{t}$ is a free parameter
that needs to be estimated, we have:
\begin{equation}
-p\log(m+n+p-1)+p\log(n_{t}+p-1)+\frac{1}{N}\sum_{i}^{N}\log|W_{i}|-\log\det\left|\frac{1}{N}\sum_{i}^{N}W_{i}\right|+\psi_{p}(\frac{1}{2}(m+n+p-1))-\psi_{p}(\frac{1}{2}(n_{t}+p-1))=0
\end{equation}
otherwise if $\Sigma_{t}=\Sigma$ is fixed and does not depend on
$n_{t}$:
\begin{equation}
\log\det\Sigma_{t}-\log\det\left(\frac{1}{N}\sum_{i}^{N}W_{i}\right)+\psi_{p}(\frac{1}{2}(m+n+p-1))-\psi_{p}(\frac{1}{2}(n_{t}+p-1))=0
\end{equation}

\section{Implementation}\label{sec:implement}

\section{Conclusion}\label{sec:conclusion}

\section*{Acknowledgement}
This work is not supported by any grant and has not been presented in any regional or international meetings.

\bibliographystyle{unsrtnat}
\bibliography{references}
\end{document}
