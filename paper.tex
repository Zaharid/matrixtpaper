\documentclass[english,listof=totoc]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[authoryear]{natbib}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=false]
 {hyperref}\usepackage{mathtools}

\title{ML Estimation of Matrix--$t$ Distributions with Expectation Maximization}


\author{Zahari Kassabov\thanks{TIF-UNIMI-2016-5}\\
        Dipartimento di Fisica, Universit\`a di Torino and INFN, Sezione di Torino\\
		TIF Lab, Dipartimento di Fisica, Universit\`a di Milano\\
        \texttt{kassabov@to.infn.it}\\
        \and
        Hsien-Ching Kao\\
        Wolfram Research, Inc.\\
		Champaign, Illinois 61820, USA\\
		\texttt{sp000088@gmail.com}
		}

\begin{document}

\maketitle

\begin{abstract}
We provide an expectation-maximization (EM) formulae for the maximum likelihood estimation of matrix--$t$ distributions and discuss their practical implementation. The result can be applied to problems where the parameters of a matrix--$t$ distribution need to be obtained from data, such as the design of recommendation systems and remote sensing algorithms.
\end{abstract}

\section{Introduction}
Matrix--$t$ distribution is a generalization of multivariate $t$ distribution to matrix variates situations. Similar as other $t$ distributions, matrix--$t$ distribution is a scale mixture of a particular class of multivariate normal models, matrix normal distributions, except the mixture parameter is a matrix rather than a scalar \citep{gupta1999matrix}. Due to this, the distribution shares the importance in robust estimation and has been applied to many predictive tasks such as spatial interpolation on the prediction of pollution concentration \citep{KIBRIA2006785}, recommendation systems \citep{NIPS2007_3203}, and ... In these problems, it is necessary to obtain the parameters of the distribution, which consist of a degrees of freedom parameter, a matrix location parameter, and two scale matrix parameters, that describe a set of given matrix--valued observations for further inference.

A common approach on estimating the parameters is through the method of maximum likelihood (ML). However, optimizing the likelihood directly is a formidable task as the gradient equations are unwieldy slow to compute and solve. For $t$ distributions, it is well-known that one can exploit the property of scale mixture and apply expectation maximization (EM) technique to optimize the likelihood function \citep{10.2307/24305551}. The same trick can be also applied to matrix--$t$ distribution by viewing it as a matrix-normal distribution with inverse Wishart distributed scale parameter.

This note is organized as follows. In section \ref{sec:matrixt} we give a brief review on matrix--$t$ distributions. The notations and the preliminary results on matrix calculus needed to present the derivation are presented sequentially in the section. EM formulae together with the relevant derivation and implementation details are provided in section \ref{sec:EM} and a brief conclusion is given in section \ref{sec:conclusion}.

\section{Matrix--$t$ distributions}\label{sec:matrixt}

All matrices referred in this note are real. For notation convenience, the matrix determinant is denoted as $|\cdot|$ and $\mathbf{S}_+^p$ is used to indicate the set of symmetric positive definite (SPD) matrices of dimension $p$. The multivariate gamma function $\Gamma_{p}(\cdot)$ given in terms of the standard gamma function $\Gamma(\cdot)$ has the form
\begin{equation}
\Gamma_{p}(x)=\pi^{p(p-1)/4}\prod_{j=1}^{p}\Gamma\left(x+\frac{1-j}{2}\right).\label{eq:multgammadef}
\end{equation}
We also define the multivariate digamma function
%
\begin{equation}
\psi_{p}(x):=\frac{d}{dx}\log\Gamma_{p}(x)=\sum_{j=1}^{p}\psi\left(x+\frac{1-j}{2}\right).
\end{equation}

A matrix--$t$ distribution $T_{p,m}(M,\Sigma,\Omega,\nu)$ of dimensions $p\times m$ is specified by 4 parameters: the location matrix $M$ of dimensions $p\times m$ , the row and column SPD scale matrices $\Sigma$ and $\Omega$ of dimensions $p\times p$ and $m\times m$, respectively, and the degrees of freedom parameter $\nu>0$. A $p \times m$ random matrix $T$ is said to be matrix--$t$ distributed if given the scale parameter $S$, a SPD matrix of dimensions $p\times p$, the matrix $T$ is matrix normal distributed $N_{p,m}(M,S,\Omega)$ and $S$ is itself inverse Wishart distributed $W^{-1}(\Sigma,\nu +p -1)$:
\begin{equation}
T\,|\,M,\Sigma,\Omega,\nu,S \sim N_{p,m}(M,S,\Omega)\label{eq:tdef1}
\end{equation}
and
\begin{equation}
S\,|\,\Sigma,\nu \sim W^{-1}(\Sigma,\nu+p-1).\label{eq:tdef2}
\end{equation}
The matrix normal distribution $N_{p,m}(M,S,\Omega)$ has density
\begin{equation}
\frac{1}{(2\pi)^{pm/2}|\Omega|^{p/2}|S|^{m/2}}\exp\left(-\frac{1}{2}\textrm{tr}\left[\Omega^{-1}(X-M)^{\textrm{t}}S^{-1}(X-M)\right]\right)\label{eq:ndistpdf}
\end{equation}
at $X$ while the inverse Wishart distribution $W^{-1}(\Sigma,n)$ with $n>p-1$ has density
\begin{equation}
\frac{|\Sigma|^{\frac{n}{2}}}{2^{\frac{n p}{2}}|S|^{\frac{n+p+1}{2}}\Gamma_{p}(\frac{n}{2})}\exp\left(-\frac{1}{2}\textrm{tr}\left[\Sigma S^{-1}\right]\right)\label{eq:wdistpdf}
\end{equation}
at $S$. Here $\Gamma_{p}(\cdot)$ is the
multivariate gamma function defined in Eq.~\eqref{eq:multgammadef}.

Follow the definition in Eqs.~\eqref{eq:tdef1} and \eqref{eq:tdef2} and the densities given above we can see the density function $f(T;\left\{M,\Sigma,\Omega,\nu\right\})$ of a matrix--$t$ distribution $T_{p,m}(M,\Sigma,\Omega,\nu)$ satisfies
\begin{equation}
\frac{\Gamma_{p}\left(\frac{p+\nu+m-1}{2}\right)}{\Gamma_{p}\left(\frac{\nu+p-1}{2}\right)\pi^{\frac{mp}{2}}|\Omega|^{\frac{p}{2}}|\Sigma|^{\frac{m}{2}}}
|I_p+\Sigma^{-1}(T-M)\Omega^{-1}(T-M)^{\textrm{t}}|^{\frac{1-m-p-\nu}{2}}
\label{eq:matrixtpdf}
\end{equation}
with $I_p$ the identity matrix of dimension $p$. The marginalization over $S$ can be obtained by using the Jacobain determinant for inverse SPD matrix $U=S^{-1}$
\begin{equation}
\left|\frac{\partial U}{\partial S}\right|=|S|^{-p-1}
\end{equation}
and the integral identity \citep{gupta1999matrix}
\begin{equation}
\int_{\mathbf{S}_+^p}e^{-\textrm{tr}[S A]}|S|^{\alpha}\,dS=\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}},\label{eq:intmultgammadef}
\end{equation}
where $A$ is a $p\times p$ nonsingular matrix.

Similar as matrix normal distributions, matrix--$t$ distributions are defined up to a multiplicative constant, i.e., $T_{p,m}(M,c\,\Omega,\Sigma/c,\nu)$ has the same distribution as $T_{p,m}(M,\Omega,\Sigma,\nu)$ for any constant $c>0$.

\section{EM formulae for matrix--$t$ distribution}\label{sec:EM}

Given a random variable $X$ with probability distribution with density (or mass) function $p_{\theta}(\cdot)$ with $\theta$ dependence. Assume the density (or mass) function is a marginalization over some unobserved random state $Z$, i.e., $p_{\theta}(x)=\mathbb{E}_{\theta}\left[p_{\theta}(x|Z)\right]$. With observations $\left\{x_i\right\}_{i\in I}$ the classical EM method for MLE consists of two steps:

\begin{itemize}
\item At the $k$-th iteration, compute the conditional expectations with respect to $Z$
\begin{equation}
Q(\theta,\theta_k)=\sum_{i\in I}Q_i(\theta,\theta_k),\qquad Q_i(\theta,\theta_k)=\mathbb{E}_{\theta_k}\left[\log p_{\theta}(x_i,Z)|x_i\right].
\end{equation}

\item Update $\theta$ by maximizing $Q(\theta,\theta_k)$, i.e., $\theta_{k+1} = \textrm{argmax}_{\theta}\,Q(\theta,\theta_k)$.
\end{itemize}

\subsection{Expectation step}

Given observed matrix data $\left\{T_j\right\}_{j\in J}$, for each observation $T_j$ the E-step of matrix--$t$ distributions entails computing the following integral at step $k$
\begin{equation}
Q_j(\theta,\theta_k)=f(T_j;\theta_k)^{-1}\int_{\mathbf{S}_+^p}f(T_{j},S;\theta_k)\log f\left(T_{j},S;\theta\right)\,dS,\label{eq:expdef}
\end{equation}
where $\theta=\left\{M,\Sigma,\Omega,\nu\right\}$, $\theta_k=\left\{M_k,\Sigma_k,\Omega_k,\nu_k\right\}$, and $f(T,S;\theta)$ is the full likelihood of the matrix--$t$ distribution with the hidden scale parameter $S$.

We can split the integral in Eq.~\eqref{eq:expdef} by the terms of $\log f(T_{j},S;\theta)$. Eliminating a global factor $\frac{1}{2}$ and terms which do not depend on the parameters in $\theta$ we have
\begin{equation}
\begin{split}\log f(T_{j},S;\theta)\sim -\nu\log|S|-\textrm{tr}\left[S^{-1}\left\{\Sigma+(T_j-M)\Omega^{-1}(T_j-M)^{\textrm{t}}\right\}\right]\\
-\nu p\log2-p\log |\Omega|+(\nu+p-1)\log |\Sigma|-2\log\Gamma_{p}\left(\frac{\nu+p-1}{2}\right).
\end{split}\label{eq:logf}
\end{equation}
The integrals to be computed are
\begin{equation}
\int_{\mathbf{S}_+^p}f(T_j,S;\theta_k)\log |S|\,dS, \qquad 
\int_{\mathbf{S}_+^p}f(T_j,S;\theta_k) S^{-1}\,dS.
\end{equation}
They can be done by using the equalities derived by taking the derivative of Eq.~\eqref{eq:intmultgammadef} with respect to $\alpha$ and matrix $A$, respectively,
\begin{equation}
\int_{\mathbf{S}_+^p}e^{-\textrm{tr}\left[SA\right]}|S|^{\alpha}\log |S|\,dS=\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}}\left\{\gamma_{p}\left(\alpha+\frac{1+p}{2}\right)-\log |A|\right\},\label{eq:intderivative1}
\end{equation}
\begin{equation}
\int_{\mathbf{S}_+^p}e^{-\textrm{tr}\left[SA\right]}|S|^{\alpha}S\,dS=\left(\alpha+\frac{1+p}{2}\right)\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}}A^{-1}.\label{eq:intderivative2}
\end{equation}

Utilizing the equalities \eqref{eq:intderivative1} and \eqref{eq:intderivative2} with adequate rearrangements we obtain the closed form expression of $Q_j(\theta,\theta_k)$ while ignoring the irrelevant constant terms
\begin{equation}
\begin{split}Q_j(\theta,\theta_k) \sim \nu\left\{\log|W_j^{(k)}|+\psi_{p}\left(\frac{m+\nu_k+p-1}{2}\right)\right\}-2\log\Gamma_{p}\left(\frac{\nu+p-1}{2}\right)-p\log|\Omega|\\
-(m+\nu_k+p-1)\textrm{tr}\left[W_j^{(k)}\left\{\Sigma+(T_j-M)\Omega^{-1}(T_j-M)^{\textrm{t}}\right\}\right]+(\nu+p-1)\log |\Sigma|,
\end{split}
\label{eq:fsimp}
\end{equation}
where the matrices $\left\{W_j^{(k)}\right\}_{j\in J}$ are defined as
\begin{equation}
W_j^{(k)}:=\left\{\Sigma_k+(T_j-M_k)\Omega_k^{-1}(T_j-M_k)^{\textrm{t}}\right\}^{-1}.
\end{equation}
As $\Sigma_k$ and $\Omega_k$ are both SPD, each matrix $W_j^{(k)}$ is also SPD. The computation of $W_j^{(k)}$ can be done efficiently using Cholesky factorization via LAPACK routines and the BLAS routines for symmetric matrix update. When $p$ is much greater than $m$, i.e., the dimension of $\Sigma$ is much larger than $\Omega$, one can apply Sherman--Morrison--Woodbury formulae \citep{doi:10.1137/1023004} to reduce the cost of inverting the $p\times p$ matrix for each $W_j^{(k)}$. Using the formulae $W_j^{(k)}$ can be rewritten in the equivalent form
\begin{equation}
W_j^{(k)}=\Sigma_k^{-1}-\Sigma_k^{-1}U_j\left(I_m+U_j^{\textrm{t}}\Sigma_k^{-1}U_j\right)^{-1}U_j^{\textrm{t}}\Sigma_k^{-1}\label{eq:SMW}
\end{equation}
with $U_j=(T_j-M_k)\Omega_k^{-1/2}$ and $\Omega_k^{1/2}$ is the upper Cholesky factor of $\Omega_k$. Note that in Eq.~\eqref{eq:SMW} one inverts a $m\times m$ matrix for each $j$ and the inversion (or factorization) of $\Sigma_k$ only needs to be done once.

\subsection{Maximization step}

To maximize $Q(\theta,\theta_k)$ with respect to $\theta$ we compute the gradient of each parameter and equate them to $0$ to obtain $\theta_{k+1}$. For parameter $M$ we have
\begin{equation}
\sum_{j\in J}W_j^{(k)}\left(T_j-M\right)\Omega^{-1}=0.
\end{equation}
As $\Omega$ is nonsingular we obtain $M_{k+1}$ as an average of the input data weighted by $W_j^{(k)}$
\begin{equation}
M_{k+1}=\left(\sum_{j\in J}W_j^{(k)}\right)^{-1}\sum_{j\in J}W_j^{(k)}T_j.\label{eq:M}
\end{equation}
Maximizing $Q(\theta,\theta_k)$ with respect to $\Omega$ we obtain $\Omega_{k+1}$
\begin{equation}
\Omega_{k+1}=\frac{m+\nu_k+p-1}{p}\left(\frac{1}{|J|}\sum_{j\in J}(T_j-M_{k+1})^{\textrm{t}}W_j^{(k)}(T_j-M_{k+1})\right).\label{eq:Omega}
\end{equation}
For the rest of the parameter(s), we consider two possible situations.
\begin{itemize}
\item $\Sigma$ and $\nu$ are unknown: In this situation, maximizing $Q(\theta,\theta_k)$ with respect to $\Sigma$ we obtain $\Sigma_{k+1}$ in terms of the inverse of the mean of $W_j^{{(k)}}$
\begin{equation}
\Sigma_{k+1}(\nu)=\frac{\nu+p-1}{m+\nu_k+p-1}\left(\bar{W}^{(k)}\right)^{-1},\quad \bar{W}^{(k)}:=\frac{1}{|J|}\sum_{j\in J}W_j^{(k)}\label{eq:Sigma}
\end{equation}
$\Sigma_{k+1}$ has linear dependence on $\nu$. Due to this, we substitute $\Sigma_{k+1}$ back into $Q(\theta,\theta_k)$ and maximize it with repect to $\nu$. We remark that
\[\tilde{Q}(\nu):=Q\left(\left\{M_{k+1},\Sigma_{k+1}(\nu),\Omega_{k+1},\nu\right\},\theta_k\right)\]
is a concave function of $\nu$ for all $\nu>0$. The maximum only attains at positive $\nu$ if $\tilde{Q}'(0)$ is positive. For this case we obtain $\nu_{k+1}$ by solving the equation $\tilde{Q}'(\nu)=0$, which has the form
\begin{equation}
h(\nu+p-1)-h(m+\nu_k+p-1)+\frac{1}{|J|}\sum_{j\in J}\log|W_j^{(k)}|-\log\left|\bar{W}^{(k)}\right|=0\label{eq:nu1}
\end{equation}
with $h(x):=p\log(x)-\psi_p(x/2)$.
\item $\Sigma$ is known and $\nu$ is unknown: In this situation, $Q\left(\left\{M_{k+1},\Sigma,\Omega_{k+1},\nu\right\},\theta_k\right)$ is also a concave function of $\nu$. The gradient equation of $\nu$ is given by
\begin{equation}
\log|\Sigma|-\log\left|\bar{W}^{(k)}\right|+\psi_{p}\left(\frac{m+\nu_k+p-1}{2}\right)-\psi_{p}\left(\frac{\nu+p-1}{2}\right)=0.\label{eq:nu2}
\end{equation}
\end{itemize}

\section{Conclusion}\label{sec:conclusion}

\section*{Acknowledgement}
This work is not supported by any grant and has not been presented in any regional or international meetings.

\bibliographystyle{unsrtnat}
\bibliography{references}
\end{document}
