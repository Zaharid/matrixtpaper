\documentclass[english,listof=totoc]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[authoryear]{natbib}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=false]
 {hyperref}\usepackage{mathtools}

\title{ML Estimation of Matrix--$t$ Distributions with Expectation Maximization}


\author{Zahari Kassabov\thanks{TIF-UNIMI-2016-5}\\
        Dipartimento di Fisica, Universit\`a di Torino and INFN, Sezione di Torino\\
		TIF Lab, Dipartimento di Fisica, Universit\`a di Milano\\
        \texttt{kassabov@to.infn.it}\\
        \and
        Hsien-Ching Kao\\
        Wolfram Research, Inc.\\
		Champaign, Illinois 61820, USA\\
		\texttt{sp000088@gmail.com}
		}

\begin{document}

\maketitle

\begin{abstract}
We provide an expectation-maximization (EM) algorithm for the maximum likelihood estimation of matrix--$t$ distributions and discuss its practical implementation. The result can be applied to problems where the parameters of a matrix--$t$ distribution need to be obtained from data, such as the design of recommendation systems and remote sensing algorithms.
\end{abstract}

\section{Introduction}
The matrix--$t$ distribution is a generalization of the multivariate $t$ distribution to a matrix variate situation. Similar as other $t$ distributions, the matrix--$t$ distribution is a scale mixture of a particular class of multivariate normal models, the matrix normal distribution, except the mixture parameter is a matrix rather than a scalar \citep{gupta1999matrix}. Due to this, the distribution shares its importance in robust estimation and has been applied to many predictive tasks such as spatial interpolation on the prediction of pollution concentration \citep{KIBRIA2006785} and recommendation systems \citep{NIPS2007_3203}.

These matrix variate models, including matrix normal distributions \citep{2013arXiv1309.6609G}, give better predictive results than the traditional PCA methods as the correlation structure among different observations can be captured in one of the scale matrix parameter and hence do not require the $i.i.d$ assumption. In these inferential problems, it is necessary to obtain the parameters of the underlying model. For matrix--$t$ distributions, the parameters consist of a degrees of freedom parameter, a matrix location parameter, and two scale matrix parameters, which describe a set of given matrix--valued observations for further inference.

A common approach on estimating the parameters is through the method of maximum likelihood (ML). However, optimizing the likelihood directly is a formidable task as the gradient equations are unwieldy slow to compute and solve. For $t$ distributions, it is well-known that one can exploit the mixture property and apply expectation maximization (EM) technique to optimize the likelihood function \citep{10.2307/24305551}. The same technique can be also applied to the matrix variate situation by viewing it as a matrix-normal distribution with inverse Wishart distributed scale parameter. When missing data presents, imputation of the missing values can be considered as a prediction problem and the relevant work for matrix--$t$ distributions can be found in \cite{NIPS2007_3203}.

This paper is organized as follows. In section \ref{sec:matrixt} we give a brief review on matrix--$t$ distributions. Notations and preliminary matrix calculus results needed to present the derivation are presented sequentially in the section. EM formulae together with the relevant derivation and implementation details are provided in section \ref{sec:EM} and a brief discussion is given in section \ref{sec:discussion}.

\section{Matrix--$t$ distributions}\label{sec:matrixt}

All matrices referred in this paper are real. For notation convenience, the matrix determinant is denoted as $|\cdot|$ and $\mathbf{S}_{++}^p$ is used to indicate the set of symmetric positive definite (SPD) matrices of dimension $p$. The multivariate gamma function $\Gamma_{p}(\cdot)$ given in terms of the standard gamma function $\Gamma(\cdot)$ has the form
\begin{equation}
\Gamma_{p}(x)=\pi^{p(p-1)/4}\prod_{j=1}^{p}\Gamma\left(x+\frac{1-j}{2}\right).\label{eq:multgammadef}
\end{equation}
We also define the multivariate digamma function
%
\begin{equation}
\psi_{p}(x):=\frac{d}{dx}\log\Gamma_{p}(x)=\sum_{j=1}^{p}\psi\left(x+\frac{1-j}{2}\right).
\end{equation}

A matrix--$t$ distribution $T_{p,m}(M,\Sigma,\Omega,\nu)$ of dimensions $p\times m$ is specified by 4 parameters: the location matrix $M$ of dimensions $p\times m$ , the row and column SPD scale matrices $\Sigma$ and $\Omega$ of dimensions $p\times p$ and $m\times m$, respectively, and the degrees of freedom parameter $\nu>0$. A $p \times m$ random matrix $T$ is said to be matrix--$t$ distributed if given the scale parameter $S$, a SPD matrix of dimensions $p\times p$, the matrix $T$ is matrix normal distributed $N_{p,m}(M,S,\Omega)$ and $S$ is itself inverse Wishart distributed $W^{-1}(\Sigma,\nu +p -1)$:
\begin{equation}
T\,|\,M,\Sigma,\Omega,\nu,S \sim N_{p,m}(M,S,\Omega)\label{eq:tdef1}
\end{equation}
and
\begin{equation}
S\,|\,\Sigma,\nu \sim W^{-1}(\Sigma,\nu+p-1).\label{eq:tdef2}
\end{equation}
The matrix normal distribution $N_{p,m}(M,S,\Omega)$ has density
\begin{equation}
\frac{1}{(2\pi)^{pm/2}|\Omega|^{p/2}|S|^{m/2}}\exp\left(-\frac{1}{2}\textrm{tr}\left[\Omega^{-1}(X-M)^{\textrm{t}}S^{-1}(X-M)\right]\right)\label{eq:ndistpdf}
\end{equation}
at $X$ while the inverse Wishart distribution $W^{-1}(\Sigma,n)$ with $n>p-1$ has density
\begin{equation}
\frac{|\Sigma|^{\frac{n}{2}}}{2^{\frac{n p}{2}}|S|^{\frac{n+p+1}{2}}\Gamma_{p}(\frac{n}{2})}\exp\left(-\frac{1}{2}\textrm{tr}\left[\Sigma S^{-1}\right]\right)\label{eq:wdistpdf}
\end{equation}
at $S$. Here $\Gamma_{p}(\cdot)$ is the
multivariate gamma function defined in Eq.~\eqref{eq:multgammadef}.

Follow the definitions in Eqs.~\eqref{eq:tdef1} and \eqref{eq:tdef2} and the densities given above we can see the density function $f(T;\left\{M,\Sigma,\Omega,\nu\right\})$ of a matrix--$t$ distribution $T_{p,m}(M,\Sigma,\Omega,\nu)$ satisfies
\begin{equation}
\frac{\Gamma_{p}\left(\frac{p+\nu+m-1}{2}\right)}{\Gamma_{p}\left(\frac{\nu+p-1}{2}\right)\pi^{\frac{mp}{2}}|\Omega|^{\frac{p}{2}}|\Sigma|^{\frac{m}{2}}}
|I_p+\Sigma^{-1}(T-M)\Omega^{-1}(T-M)^{\textrm{t}}|^{\frac{1-m-p-\nu}{2}}
\label{eq:matrixtpdf}
\end{equation}
with $I_p$ an identity matrix of dimension $p$. The marginalization over $S$ can be obtained by using the Jacobain determinant for inverse SPD matrix $U=S^{-1}$
\begin{equation}
\left|\frac{\partial U}{\partial S}\right|=|S|^{-p-1}
\end{equation}
and the integral identity \citep{gupta1999matrix}
\begin{equation}
\int_{\mathbf{S}_{++}^p}e^{-\textrm{tr}[S A]}|S|^{\alpha}\,dS=\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}}.\label{eq:intmultgammadef}
\end{equation}
Here $A$ is a $p\times p$ nonsingular matrix.

Similar as matrix normal distributions, matrix--$t$ distributions are defined up to a multiplicative constant. It is easy to see from the density function \eqref{eq:matrixtpdf} that $T_{p,m}(M,c\,\Omega,\Sigma/c,\nu)$ has the same distribution as $T_{p,m}(M,\Omega,\Sigma,\nu)$ for any constant $c>0$. This property needs to be addressed properly in ML estimation to give a convergence iterative algorithm.

\section{EM formulae for matrix--$t$ distribution}\label{sec:EM}

Given a random variable $X$ with probability density (or mass) function $p_{\theta}(\cdot)$. Assume the density (or mass) function is a marginalization over some unobserved random state $Z$, i.e., $p_{\theta}(x)=\mathbb{E}_{\theta}\left[p_{\theta}(x|Z)\right]$. With observations $\left\{x_i\right\}_{i\in I}$ the classical EM method for ML estimation consists of two steps:

\begin{itemize}
\item At the $k$-th iteration, compute the conditional expectations with respect to $Z$ for each observation $x_i$
\begin{equation}
Q_i(\theta,\theta_k)=\mathbb{E}_{\theta_k}\left[\log p_{\theta}(x_i,Z)|x_i\right].
\end{equation}

\item Update $\theta$ by maximizing $Q(\theta,\theta_k):=\sum_{i\in I}Q_i(\theta,\theta_k)$ with respect to $\theta$, i.e.,
\begin{equation}
\theta_{k+1} = \textrm{argmax}_{\theta}\,Q(\theta,\theta_k).
\end{equation}
\end{itemize}

\subsection{Expectation step}

Given observed matrix data $\left\{T_j\right\}_{j\in J}$ for each observation $T_j$ the E-step of the matrix--$t$ distribution entails computing the following integral at step $k$
\begin{equation}
Q_j(\theta,\theta_k)=f(T_j;\theta_k)^{-1}\int_{\mathbf{S}_{++}^p}f(T_{j},S;\theta_k)\log f\left(T_{j},S;\theta\right)\,dS,\label{eq:expdef}
\end{equation}
where $\theta=\left\{M,\Sigma,\Omega,\nu\right\}$, $\theta_k=\left\{M_k,\Sigma_k,\Omega_k,\nu_k\right\}$, and $f(T,S;\theta)$ is the full likelihood of the distribution with the hidden scale parameter $S$.

We can split the integral in Eq.~\eqref{eq:expdef} by the terms of $\log f(T_{j},S;\theta)$. Eliminating a global factor $\frac{1}{2}$ and constant terms which do not depend on the parameters in $\theta$ we have
\begin{equation}
\begin{split}\log f(T_{j},S;\theta)\sim -\nu\log|S|-\textrm{tr}\left[S^{-1}\left\{\Sigma+(T_j-M)\Omega^{-1}(T_j-M)^{\textrm{t}}\right\}\right]\\
-\nu p\log2-p\log |\Omega|+(\nu+p-1)\log |\Sigma|-2\log\Gamma_{p}\left(\frac{\nu+p-1}{2}\right).
\end{split}\label{eq:logf}
\end{equation}
The expectation computation requires computing the following two integrals:
\begin{equation}
\int_{\mathbf{S}_{++}^p}f(T_j,S;\theta_k)\log |S|\,dS, \qquad 
\int_{\mathbf{S}_{++}^p}f(T_j,S;\theta_k) S^{-1}\,dS.
\end{equation}
It can be done by using equalities derived by taking the derivative of Eq.~\eqref{eq:intmultgammadef} with respect to $\alpha$ and matrix $A$, respectively, and this gives
\begin{equation}
\int_{\mathbf{S}_{++}^p}e^{-\textrm{tr}\left[SA\right]}|S|^{\alpha}\log |S|\,dS=\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}}\left\{\gamma_{p}\left(\alpha+\frac{1+p}{2}\right)-\log |A|\right\},\label{eq:intderivative1}
\end{equation}
\begin{equation}
\int_{\mathbf{S}_{++}^p}e^{-\textrm{tr}\left[SA\right]}|S|^{\alpha}S\,dS=\left(\alpha+\frac{1+p}{2}\right)\Gamma_{p}\left(\alpha+\frac{1+p}{2}\right)|A|^{-\alpha-\frac{1+p}{2}}A^{-1}.\label{eq:intderivative2}
\end{equation}

Utilizing the equalities \eqref{eq:intderivative1} and \eqref{eq:intderivative2} with adequate rearrangements we obtain the closed form expression of $Q_j(\theta,\theta_k)$ while ignoring the irrelevant constant terms
\begin{equation}
\begin{split}Q_j(\theta,\theta_k) \sim \nu\left\{\log|W_j^{(k)}|+\psi_{p}\left(\frac{m+\nu_k+p-1}{2}\right)\right\}-2\log\Gamma_{p}\left(\frac{\nu+p-1}{2}\right)-p\log|\Omega|\\
-(m+\nu_k+p-1)\textrm{tr}\left[W_j^{(k)}\left\{\Sigma+(T_j-M)\Omega^{-1}(T_j-M)^{\textrm{t}}\right\}\right]+(\nu+p-1)\log |\Sigma|.
\end{split}
\label{eq:fsimp}
\end{equation}
Here the matrices $\left\{W_j^{(k)}\right\}_{j\in J}$ are defined as
\begin{equation}
W_j^{(k)}:=\left\{\Sigma_k+(T_j-M_k)\Omega_k^{-1}(T_j-M_k)^{\textrm{t}}\right\}^{-1}.
\end{equation}

As $\Sigma_k$ and $\Omega_k$ are both SPD, each matrix $W_j^{(k)}$ is also SPD. The computation of $W_j^{(k)}$ can be done efficiently using Cholesky factorization via LAPACK routines and the BLAS routines for symmetric matrix update. When $p$ is much greater than $m$, i.e., the dimension of $\Sigma$ is much larger than $\Omega$, one can apply Sherman--Morrison--Woodbury formulae \citep{doi:10.1137/1023004} to reduce the cost of inverting the $p\times p$ matrix for each $W_j^{(k)}$. By using the formulae $W_j^{(k)}$ can be rewritten in the following equivalent form
\begin{equation}
W_j^{(k)}=\Sigma_k^{-1}-\Sigma_k^{-1}U_j\left(I_m+U_j^{\textrm{t}}\Sigma_k^{-1}U_j\right)^{-1}U_j^{\textrm{t}}\Sigma_k^{-1}\label{eq:SMW}
\end{equation}
with $U_j=(T_j-M_k)\Omega_k^{-1/2}$ and $\Omega_k^{1/2}$ is the upper Cholesky factor of $\Omega_k$. Note that in Eq.~\eqref{eq:SMW} one inverts a $m\times m$ matrix for each $j$ and the inversion (or factorization) of $\Sigma_k$ only needs to be done once in each iteration.

\subsection{Maximization step}

To maximize $Q(\theta,\theta_k)$ with respect to $\theta$ we compute the gradient of each parameter and equate them to $0$ to obtain $\theta_{k+1}$. For parameter $M$ we have
\begin{equation}
\sum_{j\in J}W_j^{(k)}\left(T_j-M\right)\Omega^{-1}=0.
\end{equation}
As $\Omega$ is nonsingular we obtain $M_{k+1}$ as an average of the input data weighted by $W_j^{(k)}$
\begin{equation}
M_{k+1}=\left(\sum_{j\in J}W_j^{(k)}\right)^{-1}\sum_{j\in J}W_j^{(k)}T_j.\label{eq:M}
\end{equation}
Maximizing $Q(\theta,\theta_k)$ with respect to $\Omega$ we obtain $\Omega_{k+1}$
\begin{equation}
\Omega_{k+1}=\frac{m+\nu_k+p-1}{p}\left(\frac{1}{|J|}\sum_{j\in J}(T_j-M_{k+1})^{\textrm{t}}W_j^{(k)}(T_j-M_{k+1})\right).\label{eq:Omega}
\end{equation}
For the rest of the parameter(s), we consider two possible situations.
\begin{itemize}
\item $\Sigma$ and $\nu$ are unknown: In this situation, maximizing $Q(\theta,\theta_k)$ with respect to $\Sigma$ we obtain $\Sigma_{k+1}$ in terms of the inverse of the mean of $W_j^{{(k)}}$
\begin{equation}
\Sigma_{k+1}(\nu)=\frac{\nu+p-1}{m+\nu_k+p-1}\left(\bar{W}^{(k)}\right)^{-1},\quad \bar{W}^{(k)}:=\frac{1}{|J|}\sum_{j\in J}W_j^{(k)}.\label{eq:Sigma}
\end{equation}
$\Sigma_{k+1}$ depends linearly on $\nu$. Due to this, we substitute $\Sigma_{k+1}$ back into $Q(\theta,\theta_k)$ and maximize it with repect to $\nu$. We remark that
\[\tilde{Q}(\nu):=Q\left(\left\{M_{k+1},\Sigma_{k+1}(\nu),\Omega_{k+1},\nu\right\},\theta_k\right)\]
is a concave function of $\nu$ for all $\nu>0$. The maximum only attains at positive $\nu$ if $\tilde{Q}'(0)$ is positive. In this situation we obtain $\nu_{k+1}$ by solving the equation $\tilde{Q}'(\nu)=0$ which has the form
\begin{equation}
h(\nu+p-1)-h(m+\nu_k+p-1)+\frac{1}{|J|}\sum_{j\in J}\log|W_j^{(k)}|-\log |\bar{W}^{(k)}|=0\label{eq:nu1}
\end{equation}
with $h(x):=p\log(x)-\psi_p(x/2)$.
\item $\Sigma$ is known and $\nu$ is unknown: In this situation, $Q\left(\left\{M_{k+1},\Sigma,\Omega_{k+1},\nu\right\},\theta_k\right)$ is also a concave function of $\nu$. The gradient equation of $\nu$ is given by
\begin{equation}
\log|\Sigma|-\log |\bar{W}^{(k)}|+\psi_{p}\left(\frac{m+\nu_k+p-1}{2}\right)-\psi_{p}\left(\frac{\nu+p-1}{2}\right)=0.\label{eq:nu2}
\end{equation}
\end{itemize}

When $\Omega$ and $\Sigma$ are both unknown, the updating steps given in Eqs.~\eqref{eq:Omega} and \eqref{eq:Sigma} increases the likelihood of matrix--$t$ distribution monotonically but the parameters $\Omega_k$ and $\Sigma_k$ may not converge due to the non-identifiability of matrix--$t$ mentioned at the end of section \ref{sec:matrixt}. This issue has also been discussed in \citep{2013arXiv1309.6609G} for ML estimation of matrix normal distributions. However, instead of fixing a single component in one of the scale matrices, we suggest setting the mean trace ratio
\begin{equation}
\frac{\textrm{tr}[\Omega_k]/m}{\textrm{tr}[\Sigma_k]/p}
\end{equation}
of $\Omega_k$ and $\Sigma_k$ to 1 by proper scaling at the end of each M-step. This ensures the convergence of the algorithm and also avoids possible numerical issues due to the possible large scale separation between $\Omega_k$ and $\Sigma_k$ during the iterations.

\subsection{Parameters initialization}

For EM method, the initial values for the parameters has significant effect on the convergence behavior as the algorithm does not guarantee reaching the global optimum. While this is less of an issue for matrix--$t$ distributions as the ML estimation is a convex optimization problem in parameters $\left\{M,\Sigma^{-1},\Omega^{-1},\nu\right\}$, it is still desirable to have an efficient method for selecting good initial values to reduce the number of EM iterations for reaching the required accuracy as the classical log EM is a first order algorithm.

For the location parameter $M$, the component-wise medians and spatial median \citep{Vardi15022000} with distance induced by Frobenius norm
\begin{equation}
d(T,T')=\sqrt{\textrm{tr}\left[(T-T')(T-T')^{\textrm{t}}\right]}
\end{equation}
are proper choices we found in the implementation. Both estimators can be computed with time complexity $O(Nmp)$ with $N$ the number of matrices in the given data.

For the rest of the parameters, we center the data by the initial values of $M$ and use the property of low dimensional projection of matrix--$t$ distributions to obtain the initial values. It is known that if $T\sim T_{p,m}(\mathbf{0},\Sigma,\Omega,\nu)$, then $Tv$ is multivariate $t$ distributed with scale parameter $\left(\frac{v^{\textrm{t}}\Omega v}{\nu}\right)\Sigma$ and the same degress of freedom parameter $\nu$ for any nonzero $m$ dimensional vector $v$ \citep{gupta1999matrix}. This allows using efficient routines for estimating multivariate $t$ distributions to retrieve the initial values.

\section{Discussion}\label{sec:discussion}

In this paper we derive and provide the EM formulae for ML estimation of matrix--$t$ distributions. The EM algorithm here mainly consists of computing the weight matrices $W_j^{(k)}$ and we provide some guidance on its numerical implementation. A method of choosing the initial values for the parameters in EM is briefly discussed.

In principle, the computation burden of estimating matrix--$t$ distributions can be reduced significantly with the EM algorithm discussed here. However, due to the lack of optimized routines which dedicate to linear algebra computations of matrix-valued vector components, the algorithm described here can be difficult to optimize in the actual implementation. Also, scalability with large data can be an issue as the computation involves dense matrix inversion for each observation during the EM iterations. It is possible to alleviate the cost by considering matrix--$t$ distributions with structured scale matrix parameters which allow fast factorization. The EM relations described here can still be applied but with some modifications in the M-step. The application of structured scale matrix parameters in matrix--$t$ inference will be future work.

\section*{Acknowledgement}
This work is not supported by any grant and has not been presented in any regional or international meetings.

\bibliographystyle{unsrtnat}
\bibliography{references}
\end{document}
